{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T18:31:14.762651600Z",
     "start_time": "2024-01-20T18:31:11.524683700Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "# Import models you're considering\n",
    "from Data.load_data import get_energy_data\n",
    "from helper_functions import create_error_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We want to make a regression for energy demand in germany. This task can be achieved by using gradient boosting models. They have some advantages and disadvantages (taken from [neptune.ai](https://neptune.ai/blog/gradient-boosted-decision-trees-guide)\n",
    "\n",
    "> #### Advantages of gradient boosting trees\n",
    ">- generally more accurate compare to other modes,\n",
    ">- train faster especially on larger datasets,\n",
    ">- most of them provide support handling categorical features,\n",
    ">- some of them handle missing values natively.\n",
    "\n",
    "> #### Disadvantages of gradient boosting trees\n",
    ">- prone to overfitting: this can be solved by applying L1 and L2 regularization penalties. You can try a low learning rate as well;\n",
    ">- models can be computationally expensive and take a long time to train, especially on CPUs;\n",
    "hard to interpret the final models.\n",
    "\n",
    "In gradient boosting, an ensemble of weak learners (usually, decision trees) is used to improve the performance of a machine learning model. Their combined output results in better models. \n",
    "Weak learners work sequentially: Each model tries to improve on the error from the previous model. In the case of regression, the result is the average of all weak learners.\n",
    "\n",
    "Thus, with gradient boosting models we can make a regression on energy demand and as they are described as fast training on large datasets with very good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "As the energy demand depends heavily on daily, weekly and yearly seasonality we need to add those features to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T18:31:15.864411Z",
     "start_time": "2024-01-20T18:31:14.765611200Z"
    }
   },
   "outputs": [],
   "source": [
    "df = get_energy_data()\n",
    "\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['day'] = df.index.day\n",
    "df['month'] = df.index.month\n",
    "df['day_of_year'] = df.index.dayofyear\n",
    "\n",
    "# Feature and target variable selection\n",
    "X = df[['hour', 'day_of_week', 'day', 'month', 'day_of_year', 'Temperature']]\n",
    "y = df['Load']\n",
    "\n",
    "# # Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "[Discuss any hyperparameter tuning methods you've applied, such as Grid Search or Random Search, and the rationale behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T18:31:16.448600900Z",
     "start_time": "2024-01-20T18:31:15.871466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002612 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 576\n",
      "[LightGBM] [Info] Number of data points in the train set: 20624, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 230111.775796\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 20724, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 230120.276224\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 20824, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 230108.735789\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 20924, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 230094.749909\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 577\n",
      "[LightGBM] [Info] Number of data points in the train set: 21024, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 230080.040748\n"
     ]
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=100),\n             estimator=LGBMRegressor(),\n             param_grid={'colsample_bytree': [0.3], 'learning_rate': [0.3],\n                         'max_depth': [10], 'n_estimators': [50],\n                         'num_leaves': [10]})",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=100),\n             estimator=LGBMRegressor(),\n             param_grid={&#x27;colsample_bytree&#x27;: [0.3], &#x27;learning_rate&#x27;: [0.3],\n                         &#x27;max_depth&#x27;: [10], &#x27;n_estimators&#x27;: [50],\n                         &#x27;num_leaves&#x27;: [10]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=4, test_size=100),\n             estimator=LGBMRegressor(),\n             param_grid={&#x27;colsample_bytree&#x27;: [0.3], &#x27;learning_rate&#x27;: [0.3],\n                         &#x27;max_depth&#x27;: [10], &#x27;n_estimators&#x27;: [50],\n                         &#x27;num_leaves&#x27;: [10]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_split = TimeSeriesSplit(n_splits=4, test_size=100)\n",
    "model = lightgbm.LGBMRegressor()\n",
    "parameters = {\n",
    "    # \"max_depth\": [3, 4, 6, 5, 10],\n",
    "    # \"num_leaves\": [10, 20, 30, 40, 100, 120],\n",
    "    # \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    # \"n_estimators\": [50, 100, 300, 500, 700, 900, 1000],\n",
    "    # \"colsample_bytree\": [0.3, 0.5, 0.7, 1]    \n",
    "    \"max_depth\": [10],\n",
    "    \"num_leaves\": [10],\n",
    "    \"learning_rate\": [0.3],\n",
    "    \"n_estimators\": [50],\n",
    "    \"colsample_bytree\": [0.3]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, cv=cv_split, param_grid=parameters)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement the final model(s) you've selected based on the above steps.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T18:31:16.450114800Z",
     "start_time": "2024-01-20T18:31:16.441871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the final model(s)\n",
    "# Example: model = YourChosenModel(best_hyperparameters)\n",
    "# model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "[Clearly specify which metrics you'll use to evaluate the model performance, and why you've chosen these metrics.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T18:31:16.503925200Z",
     "start_time": "2024-01-20T18:31:16.446602500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       MAE           MSE      RMSE  MAPE %   R2 %\n0  8559.43  1.265862e+08  11251.05    3.96  92.23",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAE</th>\n      <th>MSE</th>\n      <th>RMSE</th>\n      <th>MAPE %</th>\n      <th>R2 %</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8559.43</td>\n      <td>1.265862e+08</td>\n      <td>11251.05</td>\n      <td>3.96</td>\n      <td>92.23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model using your chosen metrics\n",
    "# Example for classification\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Example for regression\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "prediction = grid_search.predict(X_test)\n",
    "# Your evaluation code here\n",
    "create_error_metrics(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "[Compare the performance of your model(s) against the baseline model. Discuss any improvements or setbacks and the reasons behind them.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-20T18:31:16.504924100Z",
     "start_time": "2024-01-20T18:31:16.477389800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Comparative Analysis code (if applicable)\n",
    "# Example: comparing accuracy of the baseline model and the new model\n",
    "# print(f\"Baseline Model Accuracy: {baseline_accuracy}, New Model Accuracy: {new_model_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
