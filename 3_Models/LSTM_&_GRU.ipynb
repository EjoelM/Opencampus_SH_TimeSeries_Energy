{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Energy Consumption Forecasting\n",
        "## Table of Contents\n",
        "1. [Model Selection](#model-selection)\n",
        "2. [Feature Engineering](#feature-engineering)\n",
        "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
        "4. [Implementation](#implementation)\n",
        "\n"
      ],
      "metadata": {
        "id": "vRePRgmTAoZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "76FhzHFUUReo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5PZ1mZ0AnUj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import holidays\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection\n",
        "\n",
        "Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) are two types of Recurrent Neural Networks (RNNs) that are widely used for sequence prediction tasks, including electricity consumption prediction.\n",
        "\n",
        "* LSTM (Long Short-Term Memory) are designed to overcome the vanishing gradient problem of traditional RNNs by incorporating memory cells that can maintain information over long sequences. LSTMs are excellent at capturing temporal dependencies and patterns within time-series data, making them suitable for predicting future values based on past consumption. LSTMs can model complex nonlinear relationships in electricity usage patterns, accounting for factors like time of day, temperature, and seasonal variations.\n",
        "* GRU (Gated Recurrent Unit)\n",
        "GRUs are a variation of LSTMs with a simpler structure. GRUs have fewer parameters than LSTMs, which can lead to faster training times and reduced computational complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ga8VatKkAvo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "* The electricity Consumption is multilevel seasonality.  \n",
        "  * month $(M_i)$, where $i$ = 1 (Jan.), 2 (Feb.), ..., 11 (Nov.), 0 (Dec.)\n",
        "  * week $(W_j)$, where $j$ = 1 (Mon),..., 6 (Sat), 0 (Sun)\n",
        "  * Hours of Daylight ($HDL_t$)\n",
        "  * Holidays ($H_t$)\n",
        "  * Temperature ($T_t$)"
      ],
      "metadata": {
        "id": "ugvQfaQ0A2a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "energy_data = pd.read_csv(\"drive/MyDrive/data/energy.csv\", parse_dates=['Date (UTC)'], index_col='Date (UTC)')\n",
        "\n",
        "# Feature Engineering\n",
        "# dummy variables for month and week\n",
        "energy_data['Month'] = energy_data.index.month\n",
        "month_dummies = pd.get_dummies(energy_data['Month'], prefix='Month')\n",
        "energy_data = pd.concat([energy_data, month_dummies], axis=1)\n",
        "\n",
        "energy_data['Week'] = energy_data.index.weekday\n",
        "week_dummies = pd.get_dummies(energy_data['Week'], prefix='Week')\n",
        "energy_data = pd.concat([energy_data, week_dummies], axis=1)\n",
        "\n",
        "# HDL (Hours of Daylight)\n",
        "energy_data['d_t'] = energy_data.index.dayofyear\n",
        "# Sun's inclination angle (lambda_t) calculation\n",
        "lambda_t = 0.4102 * np.sin((2 * np.pi / 365) * (energy_data['d_t'] - 80.25))\n",
        "# Latitude for Germany\n",
        "delta = 52\n",
        "HDL = 7.722 * np.arccos(-np.tan(2 * np.pi * delta / 360) * np.tan(lambda_t))\n",
        "energy_data['HDL'] = HDL\n",
        "\n",
        "# dummy variables for holidays\n",
        "H = holidays.Germany()\n",
        "# Function to check if a date is a holiday\n",
        "def is_holiday(date):\n",
        "    return date in H\n",
        "\n",
        "energy_data['holiday'] = energy_data.index.to_series().apply(is_holiday)\n",
        "energy_data['holiday'] = energy_data['holiday'].astype(int)\n",
        "\n",
        "# select dataset for train model\n",
        "start_date = '2018-03-01'\n",
        "end_date = '2019-03-01'\n",
        "filtered_data = energy_data[(energy_data.index >= start_date) & (energy_data.index <= end_date)]\n",
        "filtered_data = filtered_data.drop(['Month', 'Week', 'd_t'], axis=1)\n",
        "features_to_scale = ['Load', 'Temperature', 'HDL']\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "# Scale the features\n",
        "filtered_data[features_to_scale] = scaler.fit_transform(filtered_data[features_to_scale])\n",
        "filtered_data.head()"
      ],
      "metadata": {
        "id": "pc_JsYDVA5Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Group 1: Feature (Load)\n",
        "* Group 2: Feature (Load, Temperature)\n",
        "* Group 3: Feature (Load, Temperature, HDL)\n",
        "* Group 4: Feature (Load, Temperature, HDL, holiday)\n",
        "* Group 5: Feature (Load, Temperature, HDL, holiday, month, week)\n",
        "### One Houe Forecasting"
      ],
      "metadata": {
        "id": "2Y_RfQkpZYLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data for LSTM and GRU\n",
        "def create_dataset(data, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps):\n",
        "        # Extract the sequence from the dataset\n",
        "        seq_x = data.iloc[i:(i + n_steps)].values  # Convert to numpy array\n",
        "        # Append the sequence to X\n",
        "        X.append(seq_x)\n",
        "        # Append the target (next value) to y\n",
        "        # Assuming the target is the first column\n",
        "        y.append(data.iloc[i + n_steps, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# set LSTM and GRU model for training\n",
        "def create_model_h(model_type, input_shape, hidden_units=50, dense_units=1, activation='linear'):\n",
        "    \"\"\"\n",
        "    Creates and returns a Sequential model with either an LSTM or GRU layer.\n",
        "\n",
        "    Parameters:\n",
        "    model_type (str): Type of the model - 'LSTM' or 'GRU'.\n",
        "    input_shape (tuple): Shape of the input data (time steps, features).\n",
        "    hidden_units (int): Number of units in the LSTM/GRU layer.\n",
        "    dense_units (int): Number of units in the Dense output layer.\n",
        "    activation (str): 'linear' is a common choice for regression problems.\n",
        "\n",
        "    Returns:\n",
        "    Sequential model\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(hidden_units, input_shape=input_shape))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(hidden_units, input_shape=input_shape))\n",
        "\n",
        "    model.add(Dense(dense_units, activation=activation))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3Vyqb-diZXkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data_1 = filtered_data[['Load']]\n",
        "filtered_data_2 = filtered_data[['Load', 'Temperature']]\n",
        "filtered_data_3 = filtered_data[['Load', 'Temperature', 'HDL']]\n",
        "filtered_data_4 = filtered_data[['Load', 'Temperature', 'HDL', 'holiday']]"
      ],
      "metadata": {
        "id": "SD6Z2ym0gHoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 1\n",
        "n_features = 1\n",
        "X, y = create_dataset(filtered_data_1, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "GFV5CNlcaJg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 1\n",
        "n_features = 2\n",
        "X, y = create_dataset(filtered_data_2, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "Q658jS0HaL5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 1\n",
        "n_features = 3\n",
        "filtered_data_3 = filtered_data[['Load', 'Temperature', 'HDL']]\n",
        "X, y = create_dataset(filtered_data_3, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "uXTzfIDtaVQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "# LSTM Loss Plot\n",
        "axes[0].plot(history_LSTM.history['loss'], label='Train Loss - LSTM')\n",
        "axes[0].plot(history_LSTM.history['val_loss'], label='Validation Loss - LSTM')\n",
        "axes[0].set_title('LSTM Model Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend(loc='upper right')\n",
        "\n",
        "# GRU Loss Plot\n",
        "axes[1].plot(history_GRU.history['loss'], label='Train Loss - GRU')\n",
        "axes[1].plot(history_GRU.history['val_loss'], label='Validation Loss - GRU')\n",
        "axes[1].set_title('GRU Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dm_qr7u_8Nse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 1\n",
        "n_features = 4\n",
        "X, y = create_dataset(filtered_data_4, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "V-tIYeqRaXJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 1\n",
        "n_features = 23\n",
        "X, y = create_dataset(filtered_data, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "Y02BmEL4aaBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Impact of Feature Complexity:\n",
        "\n",
        "Increasing the number of features introduces more complexity and potentially more noise into the models. This might be why the MSE increases for both models in the Group 4 and Group 5 scenario.\n",
        "More features do not always lead to better performance, especially if some of the added features do not have a strong predictive relationship with the target variable or are highly correlated with each other.\n",
        "* Model Sensitivity to Feature Set:\n",
        "\n",
        "The LSTM model seems to perform better with a simpler feature set, while the GRU model is more robust to the increased complexity of the feature set.\n",
        "This could be due to the inherent architectural differences between LSTM and GRU. GRUs, with fewer parameters, might be better at handling the added complexity without overfitting.\n",
        "* Feature Selection and Engineering:\n",
        "\n",
        "The results highlight the importance of careful feature selection and engineering. It suggests that merely adding more features does not guarantee better performance and can sometimes degrade the model's accuracy.\n",
        "Feature engineering should be guided by domain knowledge and feature importance analysis.\n",
        "* For one houe forecasting, month, week and holiday dummy variables do not have a strong predictive relationship with electricity consumption."
      ],
      "metadata": {
        "id": "VBnBRSSVamlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Day Forecasting"
      ],
      "metadata": {
        "id": "LJl48vwughcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, n_input_steps, n_output_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_input_steps - n_output_steps + 1):\n",
        "        seq_x = data.iloc[i:(i + n_input_steps)].values\n",
        "        seq_y = data.iloc[i + n_input_steps:i + n_input_steps + n_output_steps, 0]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def create_model_d(model_type, input_shape, hidden_units=50, dense_units=24, activation='linear'):\n",
        "    model = Sequential()\n",
        "\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(hidden_units, input_shape=input_shape))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(hidden_units, input_shape=input_shape))\n",
        "\n",
        "    model.add(Dense(dense_units, activation=activation))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wEf6mnc4hlHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 24\n",
        "n_output_steps = 24\n",
        "n_features = 1\n",
        "X, y = create_sequences(filtered_data_1, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "qearj34WhoJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 24\n",
        "n_output_steps = 24\n",
        "n_features = 2\n",
        "X, y = create_sequences(filtered_data_2, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "Xx10sFYqhvKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 24\n",
        "n_output_steps = 24\n",
        "n_features = 3\n",
        "X, y = create_sequences(filtered_data_3, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "Vt3sP5HDhy-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 24\n",
        "n_output_steps = 24\n",
        "n_features = 4\n",
        "filtered_data_4 = filtered_data[['Load', 'Temperature', 'HDL', 'holiday']]\n",
        "X, y = create_sequences(filtered_data_4, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "5eJcuiESh1z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 24\n",
        "n_output_steps = 24\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "rGiA5wkwh5Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "# LSTM Loss Plot\n",
        "axes[0].plot(history_LSTM.history['loss'], label='Train Loss - LSTM')\n",
        "axes[0].plot(history_LSTM.history['val_loss'], label='Validation Loss - LSTM')\n",
        "axes[0].set_title('LSTM Model Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend(loc='upper right')\n",
        "\n",
        "# GRU Loss Plot\n",
        "axes[1].plot(history_GRU.history['loss'], label='Train Loss - GRU')\n",
        "axes[1].plot(history_GRU.history['val_loss'], label='Validation Loss - GRU')\n",
        "axes[1].set_title('GRU Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oculiQ_28upF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* For one day forecasting, including all features lead to better performance.\n",
        "* Same situation for one week forecasting"
      ],
      "metadata": {
        "id": "73hXH22Sh-KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "Here we adjust models in two methods: n_steps and learning rate\n",
        "* n_steps\n",
        "\n",
        "n_steps refers to the number of previous time steps of data that the model uses to make a prediction. It defines the size of the input sequence window for the LSTM or GRU. A larger n_steps value allows the model to learn from longer sequences, which can be beneficial for capturing long-term dependencies in the data.\n",
        "* Learning Rate\n",
        "\n",
        "The learning rate is a hyperparameter that determines the size of the steps the model takes during the optimization process. It influences how quickly the model learns. Modern optimization algorithms like Adam adjust the learning rate during training, which can help mitigate the risks of choosing an inappropriate static learning rate.\n",
        "\n",
        "Here, we consider using learning rate decay or cyclical learning rates to dynamically adjust the learning rate during training, which can lead to better convergence properties.\n",
        "\n"
      ],
      "metadata": {
        "id": "srIMpiy0A85O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial learning rate\n",
        "initial_learning_rate = 0.01\n",
        "# Decay rate and step\n",
        "decay_steps = 50 * (len(X_train) / 32)\n",
        "decay_rate = 0.96\n",
        "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)"
      ],
      "metadata": {
        "id": "80_VZv_x4E14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Hour Forecasting\n"
      ],
      "metadata": {
        "id": "-91xSgpcm4se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 24\n",
        "n_features = 3\n",
        "X, y = create_dataset(filtered_data_3, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "3pP1SJVLnznl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 72\n",
        "n_features = 3\n",
        "X, y = create_dataset(filtered_data_3, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "JPcnE5Ejn2Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The performance of both models improves significantly compared to n_steps = 1, which is expected as they now have more context for making predictions.\n",
        "* Interestingly, the GRU model outperforms the LSTM model in this case, as indicated by its lower MSE.\n",
        "* This might suggest that for capturing longer-term dependencies (24 steps back in this case), the GRU model is more effective or efficient."
      ],
      "metadata": {
        "id": "GlAvoaaGn4qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 24\n",
        "n_features = 3\n",
        "X, y = create_dataset(filtered_data_3, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "tKnuBAojoDq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_validation = model_LSTM.predict(X_val)\n",
        "gru_validation = model_GRU.predict(X_val)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(y_val, label='Actual Data', color='red', linestyle='-')\n",
        "plt.plot(lstm_validation, label='LSTM Validation', color='blue', linestyle='--')\n",
        "plt.plot(gru_validation, label='GRU Validation', color='green', linestyle='-.')\n",
        "plt.title('Comparison of LSTM and GRU Validation with Actual Data for one-hour forecasting')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Normalized Load Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OdPTtUB89d6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Improved Performance with Learning Rate Decay\n",
        "\n",
        "In both models (LSTM and GRU), implementing an exponential decay learning rate schedule led to a significant improvement in MSE. The MSE is lower for both models when using the decaying learning rate compared to a constant learning rate.\n",
        "* Impact on LSTM and GRU Models\n",
        "\n",
        "While both models benefit from the learning rate schedule, the improvement is more pronounced in the LSTM model. This indicates that the LSTM might be more sensitive to the learning rate adjustments in your specific task and dataset.\n"
      ],
      "metadata": {
        "id": "8-h2-hY9zLkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Day Forecasting"
      ],
      "metadata": {
        "id": "gww6OgkVzsaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 72\n",
        "n_output_steps = 24\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "RoSRmwjd0bAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 168\n",
        "n_output_steps = 24\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "m0hvctJh0m0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Effectiveness of Longer Sequences: Both LSTM and GRU models generally benefit from longer input sequences up to a certain point, as they can capture more contextual information from the data.\n",
        "* Diminishing Returns: There seems to be a point where adding more past data (beyond 72 hours in this case) does not significantly improve or might even slightly worsen the performance for LSTM. This could be due to the model's increasing complexity and potential overfitting.\n",
        "* Model Selection: The choice between LSTM and GRU should consider not only performance but also computational efficiency. GRUs, with fewer parameters, might be more efficient and still offer comparable performance to LSTMs, especially when dealing with longer sequences.\n",
        "* *Optimal* n_steps: The optimal number of past time steps (n_steps) depends on the specific characteristics of the time series data. It's important to find a balance between providing enough contextual information and avoiding overfitting or unnecessary computational complexity."
      ],
      "metadata": {
        "id": "BlQlplnJ01bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 72\n",
        "n_output_steps = 24\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "r6eVRUbz1BKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_validation = model_LSTM.predict(X_val)\n",
        "gru_validation = model_GRU.predict(X_val)\n",
        "y_daily_val = np.sum(y_val.reshape(-1, 24), axis=1)\n",
        "lstm_daily_val = np.sum(lstm_validation.reshape(-1, 24), axis=1)\n",
        "gru_daily_val = np.sum(gru_validation.reshape(-1, 24), axis=1)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(y_daily_val, label='Actual Data', color='red', linestyle='-')\n",
        "plt.plot(lstm_daily_val, label='LSTM Validation', color='blue', linestyle='--')\n",
        "plt.plot(gru_daily_val, label='GRU Validation', color='green', linestyle='-.')\n",
        "plt.title('Comparison of LSTM and GRU Validation with Actual Data for one-day forecasting')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Normalized Load Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TRKPKxva9KeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Week Forecasting"
      ],
      "metadata": {
        "id": "H5g7GoaV2CcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_w(model_type, input_shape, hidden_units=50, dense_units=168, activation='linear'):\n",
        "    model = Sequential()\n",
        "\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(hidden_units, input_shape=input_shape))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(hidden_units, input_shape=input_shape))\n",
        "\n",
        "    model.add(Dense(dense_units, activation=activation))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "pknqu5pD2Bh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 72\n",
        "n_output_steps = 168\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_w('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_w('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "AkzGyEeL2JQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 168\n",
        "n_output_steps = 168\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_w('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_w('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "LZVdSXra2SVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 336\n",
        "n_output_steps = 168\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_w('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_w('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "Gd_nQdim2hx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Model Sensitivity to Input Length: Both models show varying performance based on the length of the input sequence, indicating sensitivity to how much historical data is used.\n",
        "* Performance Trade-offs: There's a trade-off between input sequence length and model performance. More historical data doesn't always translate to better performance, as seen with the 168 steps result.\n",
        "* Model Efficiency: GRU, with a simpler structure, seems to handle longer sequences (336 steps) more efficiently than LSTM, which might be beneficial for computational efficiency.\n",
        "* Optimal Input Length: The optimal number of past time steps (n_steps) is not always straightforward and may depend on the specific dynamics of the electricity load data."
      ],
      "metadata": {
        "id": "yXjPgNqd2oLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "model_LSTM = create_model_w('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_w('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "Ioidq1A926wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Learning Rate Sensitivity: Both LSTM and GRU models are sensitive to the learning rate. The right learning rate can significantly improve model performance.\n",
        "* Model Performance: At a higher learning rate of 0.01, the GRU model not only surpasses its performance at the lower learning rate but also outperforms the LSTM model. This indicates that GRU might be more responsive to a higher learning rate in this context.\n",
        "* Balance in Learning Rate Selection: A higher learning rate can speed up training but may cause instability or overshooting. Conversely, a too-low learning rate might result in slow convergence or getting stuck in local minima.\n",
        "* Experimentation is Key: These results highlight the importance of hyperparameter tuning, especially for learning rates. The optimal setting can vary depending on the model architecture and the specific task."
      ],
      "metadata": {
        "id": "_QRKnFhg29VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "model_LSTM = create_model_w('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_w('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# Evaluate\n",
        "X_val, y_val = X_train[int(X_train.shape[0]*0.8):], y_train[int(y_train.shape[0]*0.8):]\n",
        "lstm_eval = model_LSTM.evaluate(X_val, y_val, verbose=0)\n",
        "gru_eval = model_GRU.evaluate(X_val, y_val, verbose=0)\n",
        "lstm_eval, gru_eval"
      ],
      "metadata": {
        "id": "Nli5EZP83FhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_validation = model_LSTM.predict(X_val)\n",
        "gru_validation = model_GRU.predict(X_val)\n",
        "\n",
        "y_daily_val = np.sum(y_val.reshape(-1, 168), axis=1)\n",
        "lstm_daily_val = np.sum(lstm_validation.reshape(-1, 168), axis=1)\n",
        "gru_daily_val = np.sum(gru_validation.reshape(-1, 168), axis=1)\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(y_daily_val, label='Actual Data', color='red', linestyle='-')\n",
        "plt.plot(lstm_daily_val, label='LSTM Validation', color='blue', linestyle='--')\n",
        "plt.plot(gru_daily_val, label='GRU Validation', color='green', linestyle='-.')\n",
        "plt.title('Comparison of LSTM and GRU Validation with Actual Data for one-week forecasting')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Normalized Load Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HrwdbsTk9lJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "\n",
        "For one hour forecasting, we set n_features = 3 ('Load', 'Temperature', 'HDL'), n_steps = 24 with dynamitic learning rate.\n"
      ],
      "metadata": {
        "id": "IeMoz6JGA_kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 24\n",
        "n_features = 3\n",
        "X, y = create_dataset(filtered_data_3, n_steps)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_h('LSTM', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_LSTM = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_h('GRU', input_shape = (n_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history_GRU = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "lstm_pre = model_LSTM.predict(X_test)\n",
        "gru_pre = model_GRU.predict(X_test)\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(y_test, label='Actual Data', color='red', linestyle='-')\n",
        "plt.plot(lstm_pre, label='LSTM Prediction', color='blue', linestyle='--')\n",
        "plt.plot(gru_pre, label='GRU Prediction', color='green', linestyle='-.')\n",
        "plt.title('Comparison of LSTM and GRU Prediction with Actual Data for one-hour forecasting')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Normalized Load Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DOviIAWV40nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slicing the data\n",
        "y_test_subset = y_test[-240:]\n",
        "lstm_pre_subset = lstm_pre[-72:]\n",
        "gru_pre_subset = gru_pre[-72:]\n",
        "\n",
        "# Preparing indices to align the plots\n",
        "index_y_test = range(len(y_test_subset))\n",
        "index_lstm_pre = range(len(y_test_subset) - 72, len(y_test_subset))\n",
        "index_gru_pre = range(len(y_test_subset) - 72, len(y_test_subset))\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(index_y_test, y_test_subset, label='Actual Data', color='red', linestyle='-')\n",
        "plt.plot(index_lstm_pre, lstm_pre_subset, label='LSTM Prediction', color='blue', linestyle='--')\n",
        "plt.plot(index_gru_pre, gru_pre_subset, label='GRU Prediction', color='green', linestyle='-.')\n",
        "\n",
        "plt.title('Comparison of LSTM and GRU Prediction with Actual Data for one-hour forecasting')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Normalized Load Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KmFTexQI-ikz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For one hour forecasting, we set n_features = 23 ('Load', 'Temperature', 'HDL', 'Month', 'Week', 'holiday'), n_steps = 72 with dynamitic learning rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "VDFsr2Vr4wcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 72\n",
        "n_output_steps = 24\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_d('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_d('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "lstm_pre = model_LSTM.predict(X_test)\n",
        "gru_pre = model_GRU.predict(X_test)\n",
        "\n",
        "y_daily_pre = np.sum(y_test.reshape(-1, 24), axis=1)\n",
        "lstm_daily_pre = np.sum(lstm_pre.reshape(-1, 24), axis=1)\n",
        "gru_daily_pre = np.sum(gru_pre.reshape(-1, 24), axis=1)\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(y_daily_pre, label='Actual Data', color='red', linestyle='-')\n",
        "plt.plot(lstm_daily_pre, label='LSTM Prediction', color='blue', linestyle='--')\n",
        "plt.plot(gru_daily_pre, label='GRU Prediction', color='green', linestyle='-.')\n",
        "plt.title('Comparison of LSTM and GRU Prediction with Actual Data for one-day forecasting')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Normalized Load Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F_tq_rap390h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For one hour forecasting, we set n_features = 23 ('Load', 'Temperature', 'HDL', 'Month', 'Week', 'holiday'), n_steps = 336 with dynamitic learning rate."
      ],
      "metadata": {
        "id": "fg8wl5JU4x1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_input_steps = 336\n",
        "n_output_steps = 168\n",
        "n_features = 23\n",
        "X, y = create_sequences(filtered_data, n_input_steps, n_output_steps)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# LSTM\n",
        "model_LSTM = create_model_w('LSTM', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_LSTM.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_LSTM.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "# GRU\n",
        "model_GRU = create_model_w('GRU', input_shape = (n_input_steps, n_features))\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "model_GRU.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "history = model_GRU.fit(X_train, y_train, epochs=100, validation_split=0.2, verbose=0, batch_size=32)\n",
        "\n",
        "lstm_pre = model_LSTM.predict(X_test)\n",
        "gru_pre = model_GRU.predict(X_test)\n",
        "\n",
        "y_daily_pre = np.sum(y_test.reshape(-1, 168), axis=1)\n",
        "lstm_daily_pre = np.sum(lstm_pre.reshape(-1, 168), axis=1)\n",
        "gru_daily_pre = np.sum(gru_pre.reshape(-1, 168), axis=1)\n",
        "# Plotting\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.plot(y_daily_pre, label='Actual Data', color='red', linestyle='-')\n",
        "plt.plot(lstm_daily_pre, label='LSTM Prediction', color='blue', linestyle='--')\n",
        "plt.plot(gru_daily_pre, label='GRU Prediction', color='green', linestyle='-.')\n",
        "plt.title('Comparison of LSTM and GRU Prediction with Actual Data for one-week forecasting')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Normalized Load Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JKaomn7T6MnK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}